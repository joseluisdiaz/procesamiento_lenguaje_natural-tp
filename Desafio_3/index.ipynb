{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Modelo de lenguaje con tokenizaci√≥n por caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv5PEwGzZA9-"
   },
   "source": [
    "### Entrenamiento\n",
    "\n",
    "El entranamiento se hizo utilizando el script \"train.py\" ver `index.md` para ver resultados del entranmiento y cono se realizo el mismo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCeMWWupxN1-"
   },
   "source": [
    "### Generaci√≥n de secuencias"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T22:37:14.361533Z",
     "start_time": "2025-12-10T22:37:12.172332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import os"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 22:37:12.417971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T00:01:12.112885Z",
     "start_time": "2025-12-11T00:01:12.109943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hidratar_diccionarios(vocab_path='vocab.pkl'):\n",
    "    \"\"\"\n",
    "    Carga los diccionarios, inyect√°ndolos en el entorno para\n",
    "    que las funciones de inferencia puedan usarlos directamente.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(vocab_path):\n",
    "        print(f\"‚ö†Ô∏è ERROR: No se encuentran {vocab_path}\")\n",
    "        print(\"Aseg√∫rate de subir los archivos generados por train.py al entorno del notebook.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"Cargando vocabulario desde {vocab_path}...\")\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab_data = pickle.load(f)\n",
    "\n",
    "    char2idx = vocab_data['char2idx']\n",
    "    idx2char = vocab_data['idx2char']\n",
    "\n",
    "    print(\"‚úÖ ¬°Entorno hidratado correctamente!\")\n",
    "    return char2idx, idx2char\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T00:01:24.281341Z",
     "start_time": "2025-12-11T00:01:24.072509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- EJECUTAR CARGA ---\n",
    "# Esto define las variables globales char2idx e idx2char\n",
    "char2idx, idx2char = hidratar_diccionarios()\n",
    "model_simplernn = load_model('simplernn_best_model.keras')\n",
    "model_gru = load_model('gru_best_model.keras')\n",
    "model_lstm = load_model('lstm_best_model.keras')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando vocabulario desde vocab.pkl...\n",
      "‚úÖ ¬°Entorno hidratado correctamente!\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bwbS_pfhxvB3",
    "ExecuteTime": {
     "end_time": "2025-12-11T00:00:37.228688Z",
     "start_time": "2025-12-11T00:00:37.223577Z"
    }
   },
   "source": [
    "def generate_seq(model, seed_text, max_length, n_words):\n",
    "    \"\"\"\n",
    "        Exec model sequence prediction\n",
    "\n",
    "        Args:\n",
    "            model (keras): modelo entrenado\n",
    "            seed_text (string): texto de entrada (input_seq)\n",
    "            max_length (int): m√°xima longitud de la sequencia de entrada\n",
    "            n_words (int): n√∫meros de caracteres a agregar a la sequencia de entrada\n",
    "        returns:\n",
    "            output_text (string): sentencia con las \"n_words\" agregadas\n",
    "    \"\"\"\n",
    "    output_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "\t\t# Encodeamos\n",
    "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
    "\t\t# Si tienen distinto largo\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "\t\t# Predicci√≥n softmax\n",
    "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
    "\t\t# Vamos concatenando las predicciones\n",
    "        out_word = ''\n",
    "\n",
    "        out_word = idx2char[y_hat]\n",
    "\n",
    "\t\t# Agrego las palabras a la frase predicha\n",
    "        output_text += out_word\n",
    "    return output_text"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Definimos los modelos en un diccionario\n",
    "modelos = {\n",
    "    \"SimpleRNN\": model_simplernn,\n",
    "    \"GRU\": model_gru,\n",
    "    \"LSTM\": model_lstm\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JoFqRC5pxzqS",
    "ExecuteTime": {
     "end_time": "2025-12-11T19:58:13.465200Z",
     "start_time": "2025-12-11T19:57:56.970817Z"
    }
   },
   "source": [
    "seed = 'habia una vez'\n",
    "n_words = 100\n",
    "max_length = 100\n",
    "\n",
    "print(f\"--- SEMILLA: '{seed}' ---\\n\")\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    print(f\"üîµ Modelo: {nombre}\")\n",
    "    texto = generate_seq(modelo, seed, max_length, n_words)\n",
    "    print(f\"Generado: ...{texto[len(seed):]}\")\n",
    "    print(f\"Full:     {texto}\")\n",
    "    print(\"-\" * 50) # Separador visual"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SEMILLA: 'habia una vez' ---\n",
      "\n",
      "üîµ Modelo: SimpleRNN\n",
      "Generado: ... en el camino de la carca de la carca de la carca de la carca de la carca de la carca de la carca de\n",
      "Full:     habia una vez en el camino de la carca de la carca de la carca de la carca de la carca de la carca de la carca de\n",
      "--------------------------------------------------\n",
      "üîµ Modelo: GRU\n",
      "Generado: ... en el camino de la ma√±ana, se hab√≠a despu√©s de haber ser \n",
      "visto a la estaci√≥n de la ma√±ana, se hab√≠\n",
      "Full:     habia una vez en el camino de la ma√±ana, se hab√≠a despu√©s de haber ser \n",
      "visto a la estaci√≥n de la ma√±ana, se hab√≠\n",
      "--------------------------------------------------\n",
      "üîµ Modelo: LSTM\n",
      "Generado: ... en el camino de la ma√±ana, el tren se hab√≠a desaparecido en \n",
      "consiguiente, el tren se hab√≠a desapar\n",
      "Full:     habia una vez en el camino de la ma√±ana, el tren se hab√≠a desaparecido en \n",
      "consiguiente, el tren se hab√≠a desapar\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SimpleRNN entr√≥ en loop como era de esperar (memoria corta) y GRU y LSTM aprendio algo m√°s sobre el libro."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drJ6xn5qW1Hl"
   },
   "source": [
    "###  Beam search y muestreo aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_vovn9XZW1Hl",
    "ExecuteTime": {
     "end_time": "2025-12-10T23:50:51.385643Z",
     "start_time": "2025-12-10T23:50:51.382935Z"
    }
   },
   "source": [
    "# funcionalidades para hacer encoding y decoding\n",
    "\n",
    "def encode(text, max_length=100):\n",
    "\n",
    "    encoded = [char2idx[ch] for ch in text]\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def decode(seq):\n",
    "    return ''.join([idx2char[ch] for ch in seq])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I_lZiQwkW1Hl",
    "ExecuteTime": {
     "end_time": "2025-12-10T23:50:54.218385Z",
     "start_time": "2025-12-10T23:50:54.210938Z"
    }
   },
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# funci√≥n que selecciona candidatos para el beam search\n",
    "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
    "\n",
    "  # colectar todas las probabilidades para la siguiente b√∫squeda\n",
    "  pred_large = []\n",
    "\n",
    "  for idx,pp in enumerate(pred):\n",
    "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
    "\n",
    "  pred_large = np.array(pred_large)\n",
    "\n",
    "  # criterio de selecci√≥n\n",
    "  if mode == 'det':\n",
    "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
    "  elif mode == 'sto':\n",
    "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
    "  else:\n",
    "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
    "\n",
    "  # traducir a √≠ndices de token en el vocabulario\n",
    "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
    "                        np.array([idx_select%vocab_size]).T),\n",
    "                      axis=1)\n",
    "\n",
    "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
    "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
    "\n",
    "\n",
    "def beam_search(model,num_beams,num_words,input,temp=1,mode='det'):\n",
    "\n",
    "    # first iteration\n",
    "\n",
    "    # encode\n",
    "    encoded = encode(input)\n",
    "\n",
    "    # first prediction\n",
    "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
    "\n",
    "    # get vocabulary size\n",
    "    vocab_size = y_hat.shape[0]\n",
    "\n",
    "    # initialize history\n",
    "    history_probs = [0]*num_beams\n",
    "    history_tokens = [encoded[0]]*num_beams\n",
    "\n",
    "    # select num_beams candidates\n",
    "    history_probs, history_tokens = select_candidates([y_hat],\n",
    "                                        num_beams,\n",
    "                                        vocab_size,\n",
    "                                        history_probs,\n",
    "                                        history_tokens,\n",
    "                                        temp,\n",
    "                                        mode)\n",
    "\n",
    "    # beam search loop\n",
    "    for i in range(num_words-1):\n",
    "\n",
    "      preds = []\n",
    "\n",
    "      for hist in history_tokens:\n",
    "\n",
    "        # actualizar secuencia de tokens\n",
    "        input_update = np.array([hist[i+1:]]).copy()\n",
    "\n",
    "        # predicci√≥n\n",
    "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
    "\n",
    "        preds.append(y_hat)\n",
    "\n",
    "      history_probs, history_tokens = select_candidates(preds,\n",
    "                                                        num_beams,\n",
    "                                                        vocab_size,\n",
    "                                                        history_probs,\n",
    "                                                        history_tokens,\n",
    "                                                        temp,\n",
    "                                                        mode)\n",
    "\n",
    "    return history_tokens[:,-(len(input)+num_words):]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GeLqAoOYW1Hm",
    "ExecuteTime": {
     "end_time": "2025-12-11T20:27:07.321994Z",
     "start_time": "2025-12-11T20:19:26.746487Z"
    }
   },
   "source": [
    "# Configuraci√≥n global\n",
    "seed = \"habia una vez\"\n",
    "L = 50  # Longitud a generar\n",
    "num_beams_det = 5   # Beams para determinista\n",
    "num_beams_sto = 10  # Beams para estoc√°stico (m√°s ancho ayuda a la variedad)\n",
    "temperaturas = [0.2, 0.5, 1.0, 1.2, 1.5]\n",
    "\n",
    "# Definimos tus modelos disponibles\n",
    "# Aseg√∫rate de tener estas variables ya cargadas en tu notebook\n",
    "modelos = {\n",
    "    \"SimpleRNN\": model_simplernn,\n",
    "    \"GRU\": model_gru,\n",
    "    \"LSTM\": model_lstm\n",
    "}\n",
    "\n",
    "def evaluar_generacion(nombre_modelo, modelo):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ü§ñ EVALUANDO MODELO: {nombre_modelo}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # --- 1. Beam Search Determinista ---\n",
    "    print(f\"\\nüîπ Modo: Beam Search Determinista (Beams={num_beams_det})\")\n",
    "    try:\n",
    "        secuencias = beam_search(\n",
    "            model=modelo,\n",
    "            num_beams=num_beams_det,\n",
    "            num_words=L,\n",
    "            input=seed,\n",
    "            mode='det'\n",
    "        )\n",
    "        # En determinista, el √≠ndice 0 es el de mayor probabilidad acumulada\n",
    "        texto = decode(secuencias[0])\n",
    "        print(f\"   Generado: ...{texto}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "    # --- 2. Beam Search Estoc√°stico con Temperaturas ---\n",
    "    print(f\"\\nüîπ Modo: Beam Search Estoc√°stico\")\n",
    "\n",
    "    for temp in temperaturas:\n",
    "        try:\n",
    "            secuencias = beam_search(\n",
    "                model=modelo,\n",
    "                num_beams=num_beams_sto,\n",
    "                num_words=L,\n",
    "                input=seed,\n",
    "                temp=temp,\n",
    "                mode='sto'\n",
    "            )\n",
    "\n",
    "            # En estoc√°stico, cualquiera de los caminos es v√°lido. Mostramos el primero.\n",
    "            texto = decode(secuencias[0])\n",
    "            print(f\"   üå°Ô∏è Temp {temp}: ...{texto}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error con temp {temp}: {e}\")\n",
    "\n",
    "# --- Bucle Principal ---\n",
    "for nombre, modelo in modelos.items():\n",
    "    evaluar_generacion(nombre, modelo)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ü§ñ EVALUANDO MODELO: SimpleRNN\n",
      "============================================================\n",
      "\n",
      "üîπ Modo: Beam Search Determinista (Beams=5)\n",
      "   Generado: ...habia una vez en su compa√±ero de los viajeros que se hab√≠a comp\n",
      "\n",
      "üîπ Modo: Beam Search Estoc√°stico\n",
      "   üå°Ô∏è Temp 0.2: ...habia una vez en el conductor de la ma√±ana, sin embargo, el tre\n",
      "   üå°Ô∏è Temp 0.5: ...habia una vez en el capit√°n de la estaci√≥n de la estaci√≥n de la\n",
      "   üå°Ô∏è Temp 1.0: ...habia una vez en el vapor de esta del respondi√≥ picaporte se \n",
      "v\n",
      "   üå°Ô∏è Temp 1.2: ...habia una vez en la \n",
      "compa√±√≠a del capit√°n de la compa√±√≠a de la \n",
      "   üå°Ô∏è Temp 1.5: ...habia una vez en el puerto de la calcular del vapor de toda vis\n",
      "\n",
      "============================================================\n",
      "ü§ñ EVALUANDO MODELO: GRU\n",
      "============================================================\n",
      "\n",
      "üîπ Modo: Beam Search Determinista (Beams=5)\n",
      "   Generado: ...habia una vez en la estaci√≥n de la estaci√≥n de la estaci√≥n del \n",
      "\n",
      "üîπ Modo: Beam Search Estoc√°stico\n",
      "   üå°Ô∏è Temp 0.2: ...habia una vez en el camino de la ma√±ana, se hab√≠a despu√©s de ha\n",
      "   üå°Ô∏è Temp 0.5: ...habia una vez en la estaci√≥n de la estaci√≥n de los viajeros que\n",
      "   üå°Ô∏è Temp 1.0: ...habia una vez en su casa de los m√°s que hab√≠an comprometido el \n",
      "   üå°Ô∏è Temp 1.2: ...habia una vez de las cuales estaban algunos instantes de los pr\n",
      "   üå°Ô∏è Temp 1.5: ...habia una vez la compa√±√≠a de haber recibido que la noche, se le\n",
      "\n",
      "============================================================\n",
      "ü§ñ EVALUANDO MODELO: LSTM\n",
      "============================================================\n",
      "\n",
      "üîπ Modo: Beam Search Determinista (Beams=5)\n",
      "   Generado: ...habia una vez en el coronel proctor de los viajeros se hab√≠an d\n",
      "\n",
      "üîπ Modo: Beam Search Estoc√°stico\n",
      "   üå°Ô∏è Temp 0.2: ...habia una vez en el camino de la ma√±ana, en el camino de la \n",
      "co\n",
      "   üå°Ô∏è Temp 0.5: ...habia una vez en la ma√±ana, el tren se hab√≠a sido despu√©s de ha\n",
      "   üå°Ô∏è Temp 1.0: ...habia una vez en el camino de la ma√±ana, picaporte, sin embargo\n",
      "   üå°Ô∏è Temp 1.2: ...habia una vez en el conductor de los marineros de la ma√±ana, en\n",
      "   üå°Ô∏è Temp 1.5: ...habia una vez el inspector de la parte de la parte de la ma√±ana\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_LlqmtEW1Hn"
   },
   "source": [
    "Arquitectura: La LSTM es la m√°s robusta para este dataset, logrando capturar nombres propios y estructura sint√°ctica.\n",
    "\n",
    "Estrategia de Generaci√≥n: El Beam Search Estoc√°stico con una temperatura cercana a 1.0 produce los textos m√°s naturales, evitando los bucles repetitivos del determinista.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
