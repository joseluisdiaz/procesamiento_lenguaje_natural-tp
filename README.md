# Procesamiento de Lenguaje Natural (NLP) - Trabajos Pr谩cticos

Este repositorio contiene una colecci贸n de desaf铆os y proyectos pr谩cticos enfocados en t茅cnicas de **Procesamiento de Lenguaje Natural (NLP)**. A trav茅s de estos trabajos, se explora la evoluci贸n de los modelos de lenguaje, desde algoritmos probabil铆sticos cl谩sicos hasta arquitecturas de redes neuronales recurrentes y modelos Seq2Seq.

##  Contenido

El repositorio est谩 organizado en cuatro desaf铆os incrementales:

### 1. Clasificaci贸n de Texto con Na茂ve Bayes
**[Ver Notebook](https://github.com/joseluisdiaz/procesamiento_lenguaje_natural-tp/blob/main/Desafio_1.ipynb)**

Implementaci贸n de un modelo de clasificaci贸n de texto utilizando el algoritmo probabil铆stico **Na茂ve Bayes**.
* **Objetivo:** Clasificaci贸n de documentos/texto en categor铆as predefinidas.
* **T茅cnicas:** Preprocesamiento de texto (tokenizaci贸n, stop words), vectorizaci贸n (Bag of Words / TF-IDF) y modelado con Na茂ve Bayes.

### 2. Representaci贸n Vectorial con Word2Vec
**[Ver Notebook](https://github.com/joseluisdiaz/procesamiento_lenguaje_natural-tp/blob/main/Desafio_2.ipynb)**

Exploraci贸n de t茅cnicas de **Word Embeddings** para capturar relaciones sem谩nticas entre palabras.
* **Objetivo:** Crear y visualizar representaciones vectoriales de palabras.
* **T茅cnicas:** Entrenamiento de modelos **Word2Vec** (CBOW/Skip-gram), visualizaci贸n de espacios vectoriales y an谩lisis de similitud sem谩ntica.

### 3. Modelos de Lenguaje con RNNs (SimpleRNN, GRU y LSTM)
**[Ver Carpeta](https://github.com/joseluisdiaz/procesamiento_lenguaje_natural-tp/tree/main/Desafio_3)**

Comparativa y experimentaci贸n con diferentes arquitecturas de Redes Neuronales Recurrentes para la generaci贸n de texto o predicci贸n de secuencias.
* **Objetivo:** Analizar el rendimiento de distintas celdas recurrentes y estructurar un flujo de trabajo profesional.
* **Arquitecturas:** SimpleRNN, GRU (Gated Recurrent Unit) y LSTM (Long Short-Term Memory).
* **Highlight:** Se implement贸 una separaci贸n expl铆cita entre el **entrenamiento del modelo** y la **inferencia interactiva** para mejorar la modularidad del c贸digo.

### 4. Traductor Autom谩tico (Seq2Seq)
**[Ver Notebook](https://github.com/joseluisdiaz/procesamiento_lenguaje_natural-tp/blob/main/Desafio_4.ipynb)**

Desarrollo de un sistema de traducci贸n autom谩tica utilizando una arquitectura **Sequence-to-Sequence (Seq2Seq)**.
* **Objetivo:** Traducir oraciones de un idioma origen a un idioma destino (Ingl茅s - Espa帽ol).
* **T茅cnicas:** Arquitectura Encoder-Decoder, manejo de secuencias de longitud variable, capas de Embedding y capas recurrentes profundas.